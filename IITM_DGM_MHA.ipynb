{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CeCWk2RNHmDN",
        "outputId": "6d4bff00-1f93-43dd-f8ec-84babdf20903"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x79df428a7d70>"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "import numpy as np\n",
        "\n",
        "# Setup device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# For reproducibility\n",
        "torch.manual_seed(42)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    Implements Multi-Head Attention from scratch.\n",
        "    Configuration:\n",
        "    - d_model = 128\n",
        "    - num_heads = 4\n",
        "    - batch_size = 64\n",
        "    - seq_length = 32\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "\n",
        "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.num_heads = num_heads\n",
        "        self.d_k = d_model // num_heads  # Each head will get d_k = 128 / 4 = 32 dimensions\n",
        "\n",
        "        # Linear projections for queries, keys, and values\n",
        "        self.W_q = nn.Linear(d_model, d_model)  # (128 -> 128)\n",
        "        self.W_k = nn.Linear(d_model, d_model)\n",
        "        self.W_v = nn.Linear(d_model, d_model)\n",
        "\n",
        "        # Final output projection\n",
        "        self.W_o = nn.Linear(d_model, d_model)\n",
        "\n",
        "    def scaled_dot_product_attention(self, Q, K, V, mask=None):\n",
        "        \"\"\"\n",
        "        Q, K, V shapes: (batch_size, num_heads, seq_len, d_k) = (64, 4, 32, 32)\n",
        "        \"\"\"\n",
        "        # Compute attention scores: Q x K^T\n",
        "        # Resulting shape: (batch_size, num_heads, seq_len, seq_len) = (64, 4, 32, 32)\n",
        "        attn_scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
        "\n",
        "        # Apply attention mask if provided\n",
        "        if mask is not None:\n",
        "            attn_scores = attn_scores.masked_fill(mask == 0, -1e9)\n",
        "\n",
        "        # Softmax over last dimension (keys)\n",
        "        attn_probs = F.softmax(attn_scores, dim=-1)\n",
        "\n",
        "        # Multiply attention weights with values\n",
        "        # Output shape: (batch_size, num_heads, seq_len, d_k) = (64, 4, 32, 32)\n",
        "        output = torch.matmul(attn_probs, V)\n",
        "\n",
        "        return output\n",
        "\n",
        "    def split_heads(self, x):\n",
        "        \"\"\"\n",
        "        Split d_model into num_heads.\n",
        "        Input shape:  (batch_size, seq_len, d_model) = (64, 32, 128)\n",
        "        Output shape: (batch_size, num_heads, seq_len, d_k) = (64, 4, 32, 32)\n",
        "        \"\"\"\n",
        "        batch_size, seq_length, d_model = x.size()\n",
        "        # Reshape and transpose: (64, 32, 128) → (64, 32, 4, 32) → (64, 4, 32, 32)\n",
        "        return x.view(batch_size, seq_length, self.num_heads, self.d_k).transpose(1, 2)\n",
        "\n",
        "    def combine_heads(self, x):\n",
        "        \"\"\"\n",
        "        Combine heads back into original shape.\n",
        "        Input shape:  (batch_size, num_heads, seq_len, d_k) = (64, 4, 32, 32)\n",
        "        Output shape: (batch_size, seq_len, d_model) = (64, 32, 128)\n",
        "        \"\"\"\n",
        "        batch_size, num_heads, seq_length, d_k = x.size()\n",
        "        # Transpose and reshape: (64, 4, 32, 32) → (64, 32, 4, 32) → (64, 32, 128)\n",
        "        return x.transpose(1, 2).contiguous().view(batch_size, seq_length, self.d_model)\n",
        "\n",
        "    def forward(self, Q, K, V, mask=None):\n",
        "        \"\"\"\n",
        "        Input Q, K, V shape: (batch_size, seq_len, d_model) = (64, 32, 128)\n",
        "        Output shape:       (batch_size, seq_len, d_model) = (64, 32, 128)\n",
        "        \"\"\"\n",
        "        # Linear projections: all outputs shape (64, 32, 128)\n",
        "        Q = self.W_q(Q)\n",
        "        K = self.W_k(K)\n",
        "        V = self.W_v(V)\n",
        "\n",
        "        # Split into heads: (64, 32, 128) → (64, 4, 32, 32)\n",
        "        Q = self.split_heads(Q)\n",
        "        K = self.split_heads(K)\n",
        "        V = self.split_heads(V)\n",
        "\n",
        "        # Scaled Dot-Product Attention: (64, 4, 32, 32)\n",
        "        attn_output = self.scaled_dot_product_attention(Q, K, V, mask)\n",
        "\n",
        "        # Combine heads: (64, 4, 32, 32) → (64, 32, 128)\n",
        "        combined_output = self.combine_heads(attn_output)\n",
        "\n",
        "        # Final linear projection: (64, 32, 128)\n",
        "        output = self.W_o(combined_output)\n",
        "\n",
        "        return output"
      ],
      "metadata": {
        "id": "IyprWpixHxGf"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionWiseFeedForward(nn.Module):\n",
        "    \"\"\"\n",
        "    Implements the Position-wise Feed-Forward network.\n",
        "    Each position in the sequence is passed through the same MLP independently.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, d_model, d_ff):\n",
        "        super(PositionWiseFeedForward, self).__init__()\n",
        "\n",
        "        # First linear layer: expands dimensionality (128 → 512)\n",
        "        self.fc1 = nn.Linear(d_model, d_ff)\n",
        "\n",
        "        # Second linear layer: projects back to original dimension (512 → 128)\n",
        "        self.fc2 = nn.Linear(d_ff, d_model)\n",
        "\n",
        "        # Non-linearity in between\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Input shape : (batch_size, seq_length, d_model) = (64, 32, 128)\n",
        "        Output shape: (batch_size, seq_length, d_model) = (64, 32, 128)\n",
        "        \"\"\"\n",
        "        # Apply FFN: position-wise\n",
        "        return self.fc2(self.relu(self.fc1(x)))\n",
        "\n",
        "\n",
        "class TransformerBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    A single Transformer encoder block with:\n",
        "    - Multi-Head Attention (MHA)\n",
        "    - Feed-Forward Network (FFN)\n",
        "    - Residual connections + Layer Normalization\n",
        "    - Dropout (regularization)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
        "        super(TransformerBlock, self).__init__()\n",
        "\n",
        "        # Multi-Head Self-Attention module\n",
        "        self.attention = MultiHeadAttention(d_model, num_heads)\n",
        "\n",
        "        # Position-wise Feed-Forward Network\n",
        "        self.feed_forward = PositionWiseFeedForward(d_model, d_ff)\n",
        "\n",
        "        # Layer normalization for each sub-layer (after residual addition)\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "\n",
        "        # Dropout for regularization\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        \"\"\"\n",
        "        Input shape : (batch_size, seq_length, d_model) = (64, 32, 128)\n",
        "        Output shape: (batch_size, seq_length, d_model) = (64, 32, 128)\n",
        "        \"\"\"\n",
        "\n",
        "        # --- First Sub-layer: Multi-Head Self-Attention ---\n",
        "\n",
        "        # Self-attention: Q=K=V=x\n",
        "        attn_output = self.attention(x, x, x, mask)  # → (64, 32, 128)\n",
        "\n",
        "        # Add & Norm: residual connection + layer normalization\n",
        "        x = self.norm1(x + self.dropout(attn_output))  # → (64, 32, 128)\n",
        "\n",
        "        # --- Second Sub-layer: Feed-Forward Network ---\n",
        "\n",
        "        # Position-wise FFN\n",
        "        ff_output = self.feed_forward(x)  # → (64, 32, 128)\n",
        "\n",
        "        # Add & Norm: residual connection + layer normalization\n",
        "        x = self.norm2(x + self.dropout(ff_output))  # → (64, 32, 128)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "W5bW9bQ2ICd6"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleTransformerLM(nn.Module):\n",
        "    \"\"\"\n",
        "    A simple Transformer-based Language Model built using:\n",
        "    - Token embeddings + positional embeddings\n",
        "    - Stacked Transformer blocks (multi-head attention + FFN)\n",
        "    - Final linear layer to predict the next token (character)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, vocab_size, d_model, num_layers, num_heads, d_ff, max_seq_length, dropout=0.1):\n",
        "        super(SimpleTransformerLM, self).__init__()\n",
        "\n",
        "        # 🔹 Embedding layer to convert token indices → dense vectors of dim d_model\n",
        "        self.token_embedding = nn.Embedding(vocab_size, d_model)\n",
        "        #   Input shape: (batch_size, seq_length)\n",
        "        #   Output shape: (batch_size, seq_length, d_model)\n",
        "\n",
        "        # 🔹 Positional embedding: learnable embeddings for each position in the sequence\n",
        "        self.positional_embedding = nn.Embedding(max_seq_length, d_model)\n",
        "        #   Input: position indices [0, 1, 2, ..., max_seq_length - 1]\n",
        "        #   Output: (batch_size, seq_length, d_model)\n",
        "\n",
        "        # 🔹 Stack of Transformer blocks (each with attention + FFN)\n",
        "        self.transformer_blocks = nn.ModuleList([\n",
        "            TransformerBlock(d_model, num_heads, d_ff, dropout)\n",
        "            for _ in range(num_layers)\n",
        "        ])\n",
        "        #   This will hold `num_layers` blocks — each applied in sequence in forward()\n",
        "\n",
        "        # 🔹 Final output layer: projects each position's final hidden state → vocab logits\n",
        "        self.fc_out = nn.Linear(d_model, vocab_size)\n",
        "        #   Output shape: (batch_size, seq_length, vocab_size)\n",
        "\n",
        "        # 🔹 Dropout layer for regularization\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        # 🔹 Store sequence length limit for reference\n",
        "        self.max_seq_length = max_seq_length\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        \"\"\"\n",
        "        x:       (batch_size, seq_length) – input token indices\n",
        "        mask:    (optional) attention mask, used in decoder (not used here)\n",
        "        Returns: (batch_size, seq_length, vocab_size) – prediction logits\n",
        "        \"\"\"\n",
        "\n",
        "        batch_size, seq_length = x.size()\n",
        "\n",
        "        # 🔹 Create a tensor of positions [0, 1, 2, ..., seq_length - 1]\n",
        "        positions = torch.arange(0, seq_length).expand(batch_size, seq_length).to(x.device)\n",
        "        #   Shape: (batch_size, seq_length)\n",
        "\n",
        "        # 🔹 Embed tokens and positions, then sum and apply dropout\n",
        "        x = self.token_embedding(x) + self.positional_embedding(positions)\n",
        "        x = self.dropout(x)\n",
        "        #   Shape: (batch_size, seq_length, d_model)\n",
        "\n",
        "        # 🔹 Pass through each Transformer block sequentially\n",
        "        for block in self.transformer_blocks:\n",
        "            x = block(x, mask)\n",
        "        #   Shape stays the same throughout: (batch_size, seq_length, d_model)\n",
        "\n",
        "        # 🔹 Final projection: predict vocabulary logits at each position\n",
        "        logits = self.fc_out(x)\n",
        "        #   Output shape: (batch_size, seq_length, vocab_size)\n",
        "\n",
        "        return logits"
      ],
      "metadata": {
        "id": "FfRPzdY8IGNH"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================================\n",
        "# 🔹 Define a Simple Character-Level Corpus\n",
        "# ================================================\n",
        "\n",
        "text = \"\"\"\n",
        "Transformers have revolutionized the field of natural language processing.\n",
        "The core idea behind the Transformer is self-attention, a mechanism that allows the model to weigh the importance of different words in a sequence.\n",
        "This contrasts with previous models like RNNs, which processed words sequentially.\n",
        "The parallel nature of Transformers allows for much faster training on large datasets.\n",
        "\"\"\"\n",
        "# ➤ This multi-line string will be used to train a character-level language model.\n",
        "# ➤ Each character (including punctuation and whitespace) is treated as a token.\n",
        "\n",
        "# ================================================\n",
        "# 🔹 Create Vocabulary from Unique Characters\n",
        "# ================================================\n",
        "\n",
        "chars = sorted(list(set(text)))  # Extract all unique characters and sort them\n",
        "vocab_size = len(chars)          # Number of unique characters\n",
        "\n",
        "print(f\"Vocabulary size: {vocab_size}\")\n",
        "print(f\"Vocabulary: {''.join(chars)}\")\n",
        "\n",
        "# Example output:\n",
        "# Vocabulary size: 65\n",
        "# Vocabulary:\n",
        "# .ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
        "\n",
        "# ================================================\n",
        "# 🔹 Create Character ↔ Integer Mappings\n",
        "# ================================================\n",
        "\n",
        "stoi = {ch: i for i, ch in enumerate(chars)}  # String to integer\n",
        "itos = {i: ch for i, ch in enumerate(chars)}  # Integer to string\n",
        "\n",
        "# Lambda functions for encoding and decoding sequences\n",
        "encode = lambda s: [stoi[c] for c in s]       # String → List of ints\n",
        "decode = lambda l: ''.join([itos[i] for i in l])  # List of ints → String\n",
        "\n",
        "# ================================================\n",
        "# 🔹 Encode the Full Text into a Sequence of Integers\n",
        "# ================================================\n",
        "\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "# Shape: (total_characters,) → 1D tensor of integers representing the text\n",
        "\n",
        "# ================================================\n",
        "# 🔹 Prepare Input and Target Sequences\n",
        "# ================================================\n",
        "\n",
        "block_size = 32  # Context window size (sequence length)\n",
        "\n",
        "X, Y = [], []  # Lists to store input-output pairs\n",
        "\n",
        "for i in range(len(data) - block_size):\n",
        "    # ➤ Input sequence: current block of 32 characters\n",
        "    X.append(data[i     : i + block_size])\n",
        "\n",
        "    # ➤ Target sequence: next 32 characters (shifted by one position)\n",
        "    Y.append(data[i + 1 : i + block_size + 1])\n",
        "    # ➤ This sets up a next-character prediction task\n",
        "\n",
        "# Convert lists of tensors to a 2D tensor:\n",
        "# X shape: (num_sequences, block_size) = (num_examples, 32)\n",
        "# Y shape: same\n",
        "X = torch.stack(X)  # e.g., (435, 32)\n",
        "Y = torch.stack(Y)\n",
        "\n",
        "# ================================================\n",
        "# 🔹 Print Final Shapes\n",
        "# ================================================\n",
        "\n",
        "print(f\"Shape of input data (X): {X.shape}\")\n",
        "print(f\"Shape of target data (Y): {Y.shape}\")\n",
        "# ➤ Both should be [num_sequences, block_size]\n",
        "# ➤ Example: Shape of input data (X): torch.Size([435, 32])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0KyXa_VEIJRd",
        "outputId": "da4d13db-d26c-4ef4-8f0c-db127bd4d49f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary size: 32\n",
            "Vocabulary: \n",
            " ,-.NRTabcdefghiklmnopqrstuvwyz\n",
            "Shape of input data (X): torch.Size([362, 32])\n",
            "Shape of target data (Y): torch.Size([362, 32])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =====================================================\n",
        "# 🔹 Hyperparameters for Transformer Language Model\n",
        "# =====================================================\n",
        "\n",
        "D_MODEL = 128           # Hidden dimension of token embeddings and model output\n",
        "NUM_LAYERS = 4          # Number of stacked Transformer blocks (depth of the model)\n",
        "NUM_HEADS = 4           # Number of attention heads in Multi-Head Attention\n",
        "D_FF = 512              # Dimension of Feed-Forward Network inside Transformer block\n",
        "MAX_SEQ_LENGTH = block_size  # Maximum context window (sequence length), e.g., 32\n",
        "DROPOUT = 0.1           # Dropout probability for regularization\n",
        "LEARNING_RATE = 3e-4    # Learning rate for the optimizer\n",
        "EPOCHS = 5000           # Total number of training iterations\n",
        "BATCH_SIZE = 64         # Number of examples per mini-batch\n",
        "\n",
        "# =====================================================\n",
        "# 🔹 Model, Optimizer, and Loss Function Setup\n",
        "# =====================================================\n",
        "\n",
        "# Instantiate the model with the defined architecture and move it to device (CPU/GPU)\n",
        "model = SimpleTransformerLM(\n",
        "    vocab_size,         # Output vocabulary size (number of tokens to predict)\n",
        "    D_MODEL,            # Embedding and hidden dimensions\n",
        "    NUM_LAYERS,         # Number of Transformer blocks\n",
        "    NUM_HEADS,          # Number of attention heads\n",
        "    D_FF,               # FFN dimensionality\n",
        "    MAX_SEQ_LENGTH,     # Context size\n",
        "    DROPOUT             # Dropout rate\n",
        ").to(device)\n",
        "\n",
        "# Optimizer: AdamW is typically used for Transformers\n",
        "optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "# Loss function: CrossEntropyLoss for multi-class classification per character\n",
        "# Note: logits are raw scores; targets are integer indices\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# =====================================================\n",
        "# 🔹 Training Loop\n",
        "# =====================================================\n",
        "\n",
        "print(\"\\n--- Starting Training ---\")\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "\n",
        "    # -----------------------------------------\n",
        "    # 🔸 Step 1: Sample a random mini-batch\n",
        "    # -----------------------------------------\n",
        "    ix = torch.randint(0, X.shape[0], (BATCH_SIZE,))      # Sample random indices\n",
        "    x_batch = X[ix].to(device)                            # Input batch: shape (64, 32)\n",
        "    y_batch = Y[ix].to(device)                            # Target batch: shape (64, 32)\n",
        "\n",
        "    # -----------------------------------------\n",
        "    # 🔸 Step 2: Forward pass through the model\n",
        "    # -----------------------------------------\n",
        "    logits = model(x_batch)       # Shape: (64, 32, vocab_size)\n",
        "\n",
        "    # -----------------------------------------\n",
        "    # 🔸 Step 3: Reshape for loss computation\n",
        "    # -----------------------------------------\n",
        "    B, T, C = logits.shape        # B=batch, T=sequence length, C=classes\n",
        "    logits = logits.view(B * T, C)      # Reshape to (64*32, vocab_size)\n",
        "    targets = y_batch.view(B * T)       # Reshape to (64*32,) for CE loss\n",
        "\n",
        "    # -----------------------------------------\n",
        "    # 🔸 Step 4: Compute loss\n",
        "    # -----------------------------------------\n",
        "    loss = criterion(logits, targets)\n",
        "\n",
        "    # -----------------------------------------\n",
        "    # 🔸 Step 5: Backward pass and optimization\n",
        "    # -----------------------------------------\n",
        "    optimizer.zero_grad(set_to_none=True)  # Clear gradients (more efficient with set_to_none)\n",
        "    loss.backward()                        # Backpropagation\n",
        "    optimizer.step()                       # Update model weights\n",
        "\n",
        "    # -----------------------------------------\n",
        "    # 🔸 Step 6: Print loss periodically\n",
        "    # -----------------------------------------\n",
        "    if epoch % 500 == 0:\n",
        "        print(f\"Epoch {epoch}/{EPOCHS}, Loss: {loss.item():.4f}\")\n",
        "\n",
        "print(\"--- Training Complete ---\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8OIfhU97IN66",
        "outputId": "cfd4070f-0aad-45aa-c0a2-14c8713d51c8"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Starting Training ---\n",
            "Epoch 0/5000, Loss: 3.5758\n",
            "Epoch 500/5000, Loss: 0.0119\n",
            "Epoch 1000/5000, Loss: 0.0022\n",
            "Epoch 1500/5000, Loss: 0.0013\n",
            "Epoch 2000/5000, Loss: 0.0004\n",
            "Epoch 2500/5000, Loss: 0.0003\n",
            "Epoch 3000/5000, Loss: 0.0035\n",
            "Epoch 3500/5000, Loss: 0.0004\n",
            "Epoch 4000/5000, Loss: 0.0002\n",
            "Epoch 4500/5000, Loss: 0.0003\n",
            "--- Training Complete ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate(model, start_string, max_new_tokens, method, **kwargs):\n",
        "    \"\"\"\n",
        "    Generates text from the model using various decoding strategies.\n",
        "\n",
        "    Args:\n",
        "        model: The trained Transformer model.\n",
        "        start_string: The initial string to start generation from.\n",
        "        max_new_tokens: The maximum number of tokens to generate.\n",
        "        method: One of 'greedy', 'sample', 'top_k', 'top_p'.\n",
        "        **kwargs: Arguments for the decoding methods (temp, top_k, top_p).\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    # Get configuration from kwargs\n",
        "    temperature = kwargs.get('temp', 1.0)\n",
        "    top_k = kwargs.get('top_k', None)\n",
        "    top_p = kwargs.get('top_p', None)\n",
        "\n",
        "    # Encode the starting string\n",
        "    idx = torch.tensor(encode(start_string), dtype=torch.long, device=device).unsqueeze(0)\n",
        "\n",
        "    for _ in range(max_new_tokens):\n",
        "        # Crop context if it exceeds max_seq_length\n",
        "        idx_cond = idx if idx.size(1) <= MAX_SEQ_LENGTH else idx[:, -MAX_SEQ_LENGTH:]\n",
        "\n",
        "        with torch.no_grad():\n",
        "            logits = model(idx_cond)\n",
        "\n",
        "        # Focus only on the last time step\n",
        "        logits = logits[:, -1, :]\n",
        "\n",
        "        # --- Apply Decoding Strategy ---\n",
        "        if method == 'greedy':\n",
        "            idx_next = torch.argmax(logits, dim=-1, keepdim=True)\n",
        "        else: # For all sampling methods\n",
        "            # Apply temperature scaling\n",
        "            logits = logits / temperature\n",
        "\n",
        "            # Apply Top-k sampling\n",
        "            if top_k is not None:\n",
        "                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
        "                logits[logits < v[:, [-1]]] = -float('Inf')\n",
        "\n",
        "            # Apply Top-p (Nucleus) sampling\n",
        "            if top_p is not None:\n",
        "                sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
        "                cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n",
        "\n",
        "                # Remove tokens with cumulative probability above the threshold\n",
        "                sorted_indices_to_remove = cumulative_probs > top_p\n",
        "                # Shift the indices to the right to keep the first token above the threshold\n",
        "                sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
        "                sorted_indices_to_remove[..., 0] = 0\n",
        "\n",
        "                indices_to_remove = sorted_indices[sorted_indices_to_remove]\n",
        "                logits[:, indices_to_remove] = -float('Inf')\n",
        "\n",
        "            # Get probabilities and sample\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)\n",
        "\n",
        "        # Append the new token to the sequence\n",
        "        idx = torch.cat((idx, idx_next), dim=1)\n",
        "\n",
        "    return decode(idx[0].tolist())\n",
        "\n",
        "# --- Run Demonstrations ---\n",
        "print(\"\\n--- Starting Inference Demonstrations ---\\n\")\n",
        "start_prompt = \"The Transformer is\"\n",
        "max_tokens_to_gen = 100\n",
        "\n",
        "# 1. Greedy Decoding\n",
        "print(\"--- 1. Greedy Decoding ---\")\n",
        "print(\"Description: Always picks the token with the highest probability. Deterministic but can be repetitive.\")\n",
        "generated_text = generate(model, start_prompt, max_tokens_to_gen, method='greedy')\n",
        "print(f\"Output:\\n{generated_text}\\n\")\n",
        "\n",
        "\n",
        "# 2. Sampling with Temperature\n",
        "print(\"--- 2. Sampling with Temperature ---\")\n",
        "print(\"Description: Samples from the probability distribution. Temperature controls randomness.\")\n",
        "\n",
        "print(\"a) Low Temperature (T=0.5): More predictable, closer to greedy.\")\n",
        "torch.manual_seed(42) # for reproducibility\n",
        "generated_text = generate(model, start_prompt, max_tokens_to_gen, method='sample', temp=0.5)\n",
        "print(f\"Output:\\n{generated_text}\\n\")\n",
        "\n",
        "print(\"b) High Temperature (T=1.5): More random, creative, but can be nonsensical.\")\n",
        "torch.manual_seed(42) # for reproducibility\n",
        "generated_text = generate(model, start_prompt, max_tokens_to_gen, method='sample', temp=1.5)\n",
        "print(f\"Output:\\n{generated_text}\\n\")\n",
        "\n",
        "\n",
        "# 3. Top-k Sampling\n",
        "print(\"--- 3. Top-k Sampling ---\")\n",
        "print(\"Description: Restricts sampling to the 'k' most likely tokens. Avoids very unlikely tokens.\")\n",
        "torch.manual_seed(42) # for reproducibility\n",
        "generated_text = generate(model, start_prompt, max_tokens_to_gen, method='sample', temp=1.0, top_k=10)\n",
        "print(f\"Output (k=10):\\n{generated_text}\\n\")\n",
        "\n",
        "\n",
        "# 4. Top-p (Nucleus) Sampling\n",
        "print(\"--- 4. Top-p (Nucleus) Sampling ---\")\n",
        "print(\"Description: Samples from the smallest set of tokens whose cumulative probability exceeds 'p'. More dynamic than top-k.\")\n",
        "torch.manual_seed(42) # for reproducibility\n",
        "generated_text = generate(model, start_prompt, max_tokens_to_gen, method='sample', temp=1.0, top_p=0.90)\n",
        "print(f\"Output (p=0.90):\\n{generated_text}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8RbMS3bjIRqq",
        "outputId": "f4631e8f-205c-48ee-bf35-2b233e94e1f6"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Starting Inference Demonstrations ---\n",
            "\n",
            "--- 1. Greedy Decoding ---\n",
            "Description: Always picks the token with the highest probability. Deterministic but can be repetitive.\n",
            "Output:\n",
            "The Transformer is Thanarmely.\n",
            "The Tr allows wor bs for  fanceisteis ais lis tisetis ts like RNNs, whicousta witid wof\n",
            "\n",
            "--- 2. Sampling with Temperature ---\n",
            "Description: Samples from the probability distribution. Temperature controls randomness.\n",
            "a) Low Temperature (T=0.5): More predictable, closer to greedy.\n",
            "Output:\n",
            "The Transformer is Thase Troweh sfas the forelfel natureisf Transformers allows for much faster training on large data\n",
            "\n",
            "b) High Temperature (T=1.5): More random, creative, but can be nonsensical.\n",
            "Output:\n",
            "The Transformer isequcernse.\n",
            "This tis ite.\n",
            "Tally.\n",
            "The Transformer Tras former ms s ts fisetis sis RNs tiksts likestRNN\n",
            "\n",
            "--- 3. Top-k Sampling ---\n",
            "Description: Restricts sampling to the 'k' most likely tokens. Avoids very unlikely tokens.\n",
            "Output (k=10):\n",
            "The Transformer is Thasehey.\n",
            "TrmTr Trmer isf-rff-r Trmrs Trmf-astrortrsf-antss foniss s listis ms ms ts, s, s mon,ias \n",
            "\n",
            "--- 4. Top-p (Nucleus) Sampling ---\n",
            "Description: Samples from the smallest set of tokens whose cumulative probability exceeds 'p'. More dynamic than top-k.\n",
            "Output (p=0.90):\n",
            "The Transformer is Thase Troweh sfas the forelfel natureisf Transformers allows for much faster training on large data\n",
            "\n"
          ]
        }
      ]
    }
  ]
}