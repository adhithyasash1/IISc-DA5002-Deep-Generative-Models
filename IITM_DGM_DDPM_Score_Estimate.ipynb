{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/raid/home/chandanj/miniconda3/envs/raghava/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms, datasets, utils\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ----------------------------------------\n",
    "# 1) Hyperparameters and device setup\n",
    "# ----------------------------------------\n",
    "device = \"cuda:3\"\n",
    "\n",
    "epochs               = 50\n",
    "batch_size           = 128\n",
    "learning_rate        = 2e-4\n",
    "image_size           = 28\n",
    "num_channels         = 1\n",
    "timesteps            = 1000\n",
    "ema_decay            = 0.999\n",
    "num_inference_steps  = 100\n",
    "pc_corrector_steps   = 1\n",
    "pc_gamma             = 0.1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ----------------------------------------\n",
    "# 2) Cosine β-schedule and helpers\n",
    "# ----------------------------------------\n",
    "def cosine_beta_schedule(T, s=0.008):\n",
    "    steps = T + 1\n",
    "    x = torch.linspace(0, T, steps)\n",
    "    alphas_cumprod = torch.cos(((x/T + s)/(1+s)) * math.pi/2)**2\n",
    "    alphas_cumprod = alphas_cumprod / alphas_cumprod[0]\n",
    "    betas = 1 - (alphas_cumprod[1:] / alphas_cumprod[:-1])\n",
    "    return torch.clamp(betas, 1e-6, 0.999)\n",
    "\n",
    "betas = cosine_beta_schedule(timesteps).to(device)                   # [T]\n",
    "alphas = 1.0 - betas  # [T]\n",
    "alphas_cumprod = torch.cumprod(alphas, dim=0)                      # [T]\n",
    "alpha_cumprod_prev = torch.cat([\n",
    "    torch.tensor([1.0], device=device, dtype=alphas_cumprod.dtype),\n",
    "    alphas_cumprod[:-1]\n",
    "], dim=0)\n",
    "sqrt_alphas_cumprod = torch.sqrt(alphas_cumprod)                   # [T]\n",
    "sqrt_one_minus_alphas_cumprod = torch.sqrt(1.0 - alphas_cumprod)   # [T]\n",
    "posterior_variance = betas * (1.0 - alpha_cumprod_prev) / (1.0 - alphas_cumprod)\n",
    "posterior_variance = torch.cat([posterior_variance, betas[-1:]], dim=0)  # [T]\n",
    "\n",
    "def get_index_from_list(vals, t, x_shape):\n",
    "    out = vals.gather(0, t).reshape((t.shape[0],) + (1,)*(len(x_shape)-1))\n",
    "    return out.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------\n",
    "# 3) Sinusoidal time‐embedding\n",
    "# ----------------------------------------\n",
    "class SinusoidalPosEmb(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "    def forward(self, t):\n",
    "        half = self.dim // 2\n",
    "        emb = math.log(10000) / (half - 1)\n",
    "        emb = torch.exp(torch.arange(half, device=t.device) * -emb)\n",
    "        emb = t[:, None] * emb[None, :]\n",
    "        emb = torch.cat([emb.sin(), emb.cos()], dim=-1)\n",
    "        return emb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------\n",
    "# 4) A small U-Net for noise prediction\n",
    "# ----------------------------------------\n",
    "class DoubleConv(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, time_emb_dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(in_ch, out_ch, 3, padding=1),\n",
    "            nn.GroupNorm(8, out_ch),\n",
    "            nn.SiLU(),\n",
    "            nn.Conv2d(out_ch, out_ch, 3, padding=1),\n",
    "            nn.GroupNorm(8, out_ch),\n",
    "            nn.SiLU()\n",
    "        )\n",
    "        self.time_mlp = nn.Sequential(\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(time_emb_dim, out_ch)\n",
    "        )\n",
    "    def forward(self, x, t_emb):\n",
    "        h = self.net[0:3](x)\n",
    "        # add time embedding\n",
    "        time_emb = self.time_mlp(t_emb)[..., None, None]\n",
    "        h = h + time_emb\n",
    "        return self.net[3:](h)\n",
    "\n",
    "class Down(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, time_emb_dim):\n",
    "        super().__init__()\n",
    "        self.pool = nn.MaxPool2d(2)\n",
    "        self.conv = DoubleConv(in_ch, out_ch, time_emb_dim)\n",
    "    def forward(self, x, t):\n",
    "        return self.conv(self.pool(x), t)\n",
    "\n",
    "class Up(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, time_emb_dim):\n",
    "        super().__init__()\n",
    "        # up: [B, in_ch, H, W] → [B, out_ch, 2H, 2W]\n",
    "        self.up = nn.ConvTranspose2d(in_ch, out_ch, kernel_size=2, stride=2)\n",
    "        # conv now expects in_ch + out_ch channels (skip + up)\n",
    "        self.conv = DoubleConv(in_ch + out_ch, out_ch, time_emb_dim)\n",
    "\n",
    "    def forward(self, x, skip, t):\n",
    "        x = self.up(x)                # [B, out_ch, 2H, 2W]\n",
    "        x = torch.cat([x, skip], 1)   # [B, in_ch+out_ch, 2H, 2W]\n",
    "        return self.conv(x, t)        # maps back to [B, out_ch, 2H, 2W]\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, time_emb_dim=128):\n",
    "        super().__init__()\n",
    "        self.time_mlp = nn.Sequential(\n",
    "            SinusoidalPosEmb(time_emb_dim),\n",
    "            nn.Linear(time_emb_dim, time_emb_dim),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(time_emb_dim, time_emb_dim),\n",
    "        )\n",
    "        self.inc  = DoubleConv(num_channels, 64,  time_emb_dim)\n",
    "        self.down1= Down(64, 128, time_emb_dim)\n",
    "        self.down2= Down(128, 128, time_emb_dim)\n",
    "        self.up1  = Up(128, 64, time_emb_dim)\n",
    "        self.up2  = Up(64, 64,  time_emb_dim)\n",
    "        self.outc = nn.Conv2d(64, num_channels, 1)\n",
    "    def forward(self, x, t):\n",
    "        t_emb = self.time_mlp(t)\n",
    "        x1 = self.inc(x,        t_emb)\n",
    "        x2 = self.down1(x1,     t_emb)\n",
    "        x3 = self.down2(x2,     t_emb)\n",
    "        x  = self.up1(x3,  x2,  t_emb)\n",
    "        x  = self.up2(x,   x1,  t_emb)\n",
    "        return self.outc(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------\n",
    "# 5) EMA helper\n",
    "# ----------------------------------------\n",
    "class EMA:\n",
    "    def __init__(self, model, decay):\n",
    "        self.decay = decay\n",
    "        self.shadow = {k: v.clone().detach() for k, v in model.state_dict().items()}\n",
    "    def update(self, model):\n",
    "        for k, v in model.state_dict().items():\n",
    "            self.shadow[k] = self.decay * self.shadow[k] + (1. - self.decay) * v.clone().detach()\n",
    "    def apply_shadow(self, model):\n",
    "        model.load_state_dict(self.shadow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------\n",
    "# 6) Data loader\n",
    "# ----------------------------------------\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(image_size),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "train_ds = datasets.MNIST(root=\"./data\", train=True, download=True, transform=transform)\n",
    "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ----------------------------------------\n",
    "# 7) Model, optimizer, EMA\n",
    "# ----------------------------------------\n",
    "model = UNet().to(device)\n",
    "ema   = EMA(model, ema_decay)\n",
    "opt   = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/50: 100%|██████████| 469/469 [00:09<00:00, 48.92it/s, loss=1.36e-5] \n",
      "Epoch 2/50: 100%|██████████| 469/469 [00:09<00:00, 49.18it/s, loss=1.74e-5] \n",
      "Epoch 3/50: 100%|██████████| 469/469 [00:09<00:00, 49.23it/s, loss=0.0546]  \n",
      "Epoch 4/50: 100%|██████████| 469/469 [00:09<00:00, 49.02it/s, loss=1.39e-5] \n",
      "Epoch 5/50: 100%|██████████| 469/469 [00:09<00:00, 49.27it/s, loss=7.92e-6] \n",
      "Epoch 6/50: 100%|██████████| 469/469 [00:09<00:00, 49.16it/s, loss=1.22e-5] \n",
      "Epoch 7/50: 100%|██████████| 469/469 [00:09<00:00, 49.46it/s, loss=1.39e-5] \n",
      "Epoch 8/50: 100%|██████████| 469/469 [00:09<00:00, 49.30it/s, loss=9.36e-5] \n",
      "Epoch 9/50: 100%|██████████| 469/469 [00:09<00:00, 49.19it/s, loss=7.62e-6] \n",
      "Epoch 10/50: 100%|██████████| 469/469 [00:09<00:00, 49.42it/s, loss=0.102]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved samples/snapshot at epoch 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/50: 100%|██████████| 469/469 [00:09<00:00, 49.20it/s, loss=8.91e-6] \n",
      "Epoch 12/50: 100%|██████████| 469/469 [00:09<00:00, 48.95it/s, loss=1.54e-5] \n",
      "Epoch 13/50: 100%|██████████| 469/469 [00:09<00:00, 49.39it/s, loss=1.04e-5] \n",
      "Epoch 14/50: 100%|██████████| 469/469 [00:09<00:00, 49.20it/s, loss=4.58e-5] \n",
      "Epoch 15/50: 100%|██████████| 469/469 [00:09<00:00, 49.17it/s, loss=2.07e-5] \n",
      "Epoch 16/50: 100%|██████████| 469/469 [00:09<00:00, 49.47it/s, loss=1.2e-5]  \n",
      "Epoch 17/50: 100%|██████████| 469/469 [00:09<00:00, 49.43it/s, loss=5.19e-5] \n",
      "Epoch 18/50: 100%|██████████| 469/469 [00:09<00:00, 49.07it/s, loss=1.04e-5] \n",
      "Epoch 19/50: 100%|██████████| 469/469 [00:09<00:00, 49.39it/s, loss=1.01e-5] \n",
      "Epoch 20/50: 100%|██████████| 469/469 [00:09<00:00, 49.36it/s, loss=1.36e-5] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved samples/snapshot at epoch 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21/50: 100%|██████████| 469/469 [00:09<00:00, 49.43it/s, loss=3.5e-5]  \n",
      "Epoch 22/50: 100%|██████████| 469/469 [00:09<00:00, 49.51it/s, loss=7.16e-6] \n",
      "Epoch 23/50: 100%|██████████| 469/469 [00:09<00:00, 49.34it/s, loss=1.65e-5] \n",
      "Epoch 24/50: 100%|██████████| 469/469 [00:09<00:00, 49.21it/s, loss=9.66e-6] \n",
      "Epoch 25/50: 100%|██████████| 469/469 [00:09<00:00, 49.43it/s, loss=3.01e-5] \n",
      "Epoch 26/50: 100%|██████████| 469/469 [00:09<00:00, 49.41it/s, loss=1.7e-5]  \n",
      "Epoch 27/50: 100%|██████████| 469/469 [00:09<00:00, 49.08it/s, loss=1.15e-5] \n",
      "Epoch 28/50: 100%|██████████| 469/469 [00:09<00:00, 49.38it/s, loss=6.94e-6] \n",
      "Epoch 29/50: 100%|██████████| 469/469 [00:09<00:00, 49.47it/s, loss=1.07e-5] \n",
      "Epoch 30/50: 100%|██████████| 469/469 [00:09<00:00, 49.10it/s, loss=7.96e-6] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved samples/snapshot at epoch 30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 31/50: 100%|██████████| 469/469 [00:09<00:00, 49.18it/s, loss=9.13e-6] \n",
      "Epoch 32/50: 100%|██████████| 469/469 [00:09<00:00, 49.17it/s, loss=8.84e-6] \n",
      "Epoch 33/50: 100%|██████████| 469/469 [00:09<00:00, 49.43it/s, loss=1.63e-5] \n",
      "Epoch 34/50: 100%|██████████| 469/469 [00:09<00:00, 49.37it/s, loss=7.98e-6] \n",
      "Epoch 35/50: 100%|██████████| 469/469 [00:09<00:00, 49.01it/s, loss=9.29e-6] \n",
      "Epoch 36/50: 100%|██████████| 469/469 [00:09<00:00, 49.38it/s, loss=8.52e-6] \n",
      "Epoch 37/50: 100%|██████████| 469/469 [00:09<00:00, 49.54it/s, loss=1.21e-5] \n",
      "Epoch 38/50: 100%|██████████| 469/469 [00:09<00:00, 49.36it/s, loss=0.0716]  \n",
      "Epoch 39/50: 100%|██████████| 469/469 [00:09<00:00, 49.32it/s, loss=7.46e-6] \n",
      "Epoch 40/50: 100%|██████████| 469/469 [00:09<00:00, 49.40it/s, loss=9.14e-6] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved samples/snapshot at epoch 40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 41/50: 100%|██████████| 469/469 [00:09<00:00, 49.28it/s, loss=6.02e-5] \n",
      "Epoch 42/50: 100%|██████████| 469/469 [00:09<00:00, 49.32it/s, loss=2.57e-5] \n",
      "Epoch 43/50: 100%|██████████| 469/469 [00:09<00:00, 49.35it/s, loss=7.69e-6] \n",
      "Epoch 44/50: 100%|██████████| 469/469 [00:09<00:00, 49.38it/s, loss=8.05e-6] \n",
      "Epoch 45/50: 100%|██████████| 469/469 [00:09<00:00, 49.21it/s, loss=7.27e-6] \n",
      "Epoch 46/50: 100%|██████████| 469/469 [00:09<00:00, 49.53it/s, loss=0.0269]  \n",
      "Epoch 47/50: 100%|██████████| 469/469 [00:09<00:00, 49.25it/s, loss=1.14e-5] \n",
      "Epoch 48/50: 100%|██████████| 469/469 [00:09<00:00, 49.22it/s, loss=1.83e-5] \n",
      "Epoch 49/50: 100%|██████████| 469/469 [00:09<00:00, 49.44it/s, loss=3.05e-5] \n",
      "Epoch 50/50: 100%|██████████| 469/469 [00:09<00:00, 48.73it/s, loss=1.01e-5] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved samples/snapshot at epoch 50\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------------------\n",
    "# 8) Training loop\n",
    "# ----------------------------------------\n",
    "for epoch in range(epochs):\n",
    "    pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\")\n",
    "    for x0, _ in pbar:\n",
    "        x0 = x0.to(device)\n",
    "\n",
    "        # 1) sample random timesteps for each image\n",
    "        t = torch.randint(0, timesteps, (x0.size(0),), device=device)\n",
    "\n",
    "        # 2) sample noise and produce x_t\n",
    "        noise = torch.randn_like(x0)\n",
    "        sqrt_acp = get_index_from_list(sqrt_alphas_cumprod, t, x0.shape)\n",
    "        sqrt_1macp = get_index_from_list(sqrt_one_minus_alphas_cumprod, t, x0.shape)\n",
    "        x_t = sqrt_acp * x0 + sqrt_1macp * noise\n",
    "\n",
    "        # 3) predict noise and compute variance‐weighted loss\n",
    "        pred_noise = model(x_t, t)\n",
    "        beta_t = get_index_from_list(betas, t, x0.shape)\n",
    "        alpha_t = get_index_from_list(alphas, t, x0.shape)\n",
    "        acp_t = get_index_from_list(alphas_cumprod, t, x0.shape)\n",
    "        weight = (beta_t**2) / (alpha_t * (1 - acp_t))\n",
    "        loss = (weight * F.mse_loss(pred_noise, noise, reduction=\"none\")).mean()\n",
    "\n",
    "        # 4) optimizer step\n",
    "        opt.zero_grad(); loss.backward(); opt.step()\n",
    "        ema.update(model)\n",
    "\n",
    "        pbar.set_postfix(loss=loss.item())\n",
    "\n",
    "    # save a checkpoint and a sample grid every few epochs\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        torch.save(model.state_dict(), f\"ddpm_model_epoch{epoch+1}.pth\")\n",
    "        # sample images to visually monitor progress\n",
    "        ema.apply_shadow(model)\n",
    "        samples = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            x = torch.randn(16, num_channels, image_size, image_size, device=device)\n",
    "            for i in reversed(range(0, num_inference_steps)):\n",
    "                t_cur = torch.full((x.size(0),), i*(timesteps//num_inference_steps), device=device, dtype=torch.long)\n",
    "                # predictor step\n",
    "                eps = model(x, t_cur)\n",
    "                beta = get_index_from_list(betas, t_cur, x.shape)\n",
    "                sqrt1macp = get_index_from_list(sqrt_one_minus_alphas_cumprod, t_cur, x.shape)\n",
    "                coeff = beta / sqrt1macp\n",
    "                x = (1/torch.sqrt(get_index_from_list(alphas, t_cur, x.shape))) * (x - coeff * eps)\n",
    "                # corrector (Langevin) step\n",
    "                for _ in range(pc_corrector_steps):\n",
    "                    grad = model(x, t_cur)\n",
    "                    noise = torch.randn_like(x)\n",
    "                    scale = torch.sqrt(torch.tensor(2 * pc_gamma, device=x.device))\n",
    "                    x = x + pc_gamma * grad + scale * noise\n",
    "\n",
    "                # add noise if not last\n",
    "                if i > 0:\n",
    "                    var = get_index_from_list(posterior_variance, t_cur, x.shape)\n",
    "                    x = x + torch.sqrt(var) * torch.randn_like(x)\n",
    "\n",
    "                samples.append(x.cpu())\n",
    "\n",
    "        grid = utils.make_grid(samples[-1], nrow=4, normalize=True, value_range=(-1,1))\n",
    "        utils.save_image(grid, f\"samples_epoch{epoch+1}.png\")\n",
    "        torch.save(model.state_dict(), \"ema_model.pth\")\n",
    "        print(f\"Saved samples/snapshot at epoch {epoch+1}\")\n",
    "        model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved final samples to final_ddpm_samples.png\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------------------\n",
    "# 9) Final sampling with EMA weights\n",
    "# ----------------------------------------\n",
    "ema.apply_shadow(model)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    x = torch.randn(64, num_channels, image_size, image_size, device=device)\n",
    "    for i in reversed(range(0, num_inference_steps)):\n",
    "        t_cur = torch.full((x.size(0),), i*(timesteps//num_inference_steps), device=device, dtype=torch.long)\n",
    "        # predictor\n",
    "        eps = model(x, t_cur)\n",
    "        beta = get_index_from_list(betas, t_cur, x.shape)\n",
    "        sqrt1macp = get_index_from_list(sqrt_one_minus_alphas_cumprod, t_cur, x.shape)\n",
    "        coeff = beta / sqrt1macp\n",
    "        x = (1/torch.sqrt(get_index_from_list(alphas, t_cur, x.shape))) * (x - coeff * eps)\n",
    "        # corrector\n",
    "        for _ in range(pc_corrector_steps):\n",
    "            grad = model(x, t_cur)\n",
    "            noise = torch.randn_like(x)\n",
    "            scale = torch.sqrt(torch.tensor(2 * pc_gamma, device=x.device))\n",
    "            x = x + pc_gamma * grad + scale * noise\n",
    "        # noise injection\n",
    "        if i > 0:\n",
    "            var = get_index_from_list(posterior_variance, t_cur, x.shape)\n",
    "            x = x + torch.sqrt(var) * torch.randn_like(x)\n",
    "\n",
    "    utils.save_image(x.cpu(), \"final_ddpm_samples.png\", nrow=8, normalize=True, value_range=(-1,1))\n",
    "    print(\"Saved final samples to final_ddpm_samples.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "raghava",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
