{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5da2c853",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-18T12:26:18.197272Z",
     "iopub.status.busy": "2025-07-18T12:26:18.197045Z",
     "iopub.status.idle": "2025-07-18T12:26:29.060125Z",
     "shell.execute_reply": "2025-07-18T12:26:29.059275Z"
    },
    "papermill": {
     "duration": 10.867408,
     "end_time": "2025-07-18T12:26:29.061403",
     "exception": false,
     "start_time": "2025-07-18T12:26:18.193995",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting FID Score Calculation ---\n",
      "FID setup complete.\n"
     ]
    }
   ],
   "source": [
    "# --- 1. IMPORTS AND SETUP ---\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "from scipy.linalg import sqrtm\n",
    "import numpy as np\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "import os\n",
    "\n",
    "print(\"--- Starting FID Score Calculation ---\")\n",
    "\n",
    "class Config:\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    num_workers = 2\n",
    "    dataset_path = '/kaggle/input/dgm-animals/Animals_data/animals/animals'\n",
    "    image_size = 128\n",
    "    img_channels = 3\n",
    "    latent_dim = 100\n",
    "    embedding_dim = 100\n",
    "    features_g = 64\n",
    "    features_d = 64\n",
    "    batch_size = 64\n",
    "    num_classes = 90\n",
    "\n",
    "config = Config()\n",
    "\n",
    "class ConditionalGenerator(nn.Module):\n",
    "    def __init__(self, latent_dim, num_classes, embedding_dim, channels_img, features_g):\n",
    "        super(ConditionalGenerator, self).__init__()\n",
    "        self.embed = nn.Embedding(num_classes, embedding_dim)\n",
    "        self.net = nn.Sequential(\n",
    "            self._block(latent_dim + embedding_dim, features_g * 16, 4, 1, 0),\n",
    "            self._block(features_g * 16, features_g * 8, 4, 2, 1),\n",
    "            self._block(features_g * 8, features_g * 4, 4, 2, 1),\n",
    "            self._block(features_g * 4, features_g * 2, 4, 2, 1),\n",
    "            self._block(features_g * 2, features_g, 4, 2, 1),\n",
    "            nn.ConvTranspose2d(features_g, channels_img, kernel_size=4, stride=2, padding=1),\n",
    "            nn.Tanh())\n",
    "    def _block(self, in_c, out_c, k, s, p):\n",
    "        return nn.Sequential(nn.ConvTranspose2d(in_c, out_c, k, s, p, bias=False), nn.BatchNorm2d(out_c), nn.ReLU(True))\n",
    "    def forward(self, z, labels):\n",
    "        embedding = self.embed(labels).unsqueeze(2).unsqueeze(3)\n",
    "        x = torch.cat([z, embedding], dim=1)\n",
    "        return self.net(x)\n",
    "\n",
    "experiment_dir_to_evaluate = \"/kaggle/input/cgan_200/pytorch/default/1/cgan_D1_G7_200_epochs\"\n",
    "\n",
    "# --- FID Configuration ---\n",
    "FID_IMG_SIZE = 299\n",
    "FID_BATCH_SIZE = 32\n",
    "NUM_SAMPLES = 1000\n",
    "DEVICE = config.device\n",
    "\n",
    "# --- FID Transformations ---\n",
    "fid_transform = transforms.Compose([\n",
    "    transforms.Resize((FID_IMG_SIZE, FID_IMG_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "print(\"FID setup complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "50db0d9f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-18T12:26:29.066752Z",
     "iopub.status.busy": "2025-07-18T12:26:29.066080Z",
     "iopub.status.idle": "2025-07-18T12:26:30.379967Z",
     "shell.execute_reply": "2025-07-18T12:26:30.379076Z"
    },
    "papermill": {
     "duration": 1.317488,
     "end_time": "2025-07-18T12:26:30.381213",
     "exception": false,
     "start_time": "2025-07-18T12:26:29.063725",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/inception_v3_google-0cc3c7bd.pth\" to /root/.cache/torch/hub/checkpoints/inception_v3_google-0cc3c7bd.pth\n",
      "100%|██████████| 104M/104M [00:00<00:00, 226MB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "InceptionV3 model loaded.\n"
     ]
    }
   ],
   "source": [
    "class InceptionV3FeatureExtractor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = models.inception_v3(weights=models.Inception_V3_Weights.DEFAULT)\n",
    "        # Replace the final classification layer with an identity layer to get features\n",
    "        self.model.fc = nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # In eval mode, InceptionV3 has a single output.\n",
    "        # In train mode, it has an auxiliary output we would need to handle.\n",
    "        # Setting .eval() is crucial.\n",
    "        return self.model(x)\n",
    "\n",
    "inception_model = InceptionV3FeatureExtractor().to(DEVICE)\n",
    "inception_model.eval() # Must be in eval mode\n",
    "print(\"InceptionV3 model loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0b110e56",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-18T12:26:30.386791Z",
     "iopub.status.busy": "2025-07-18T12:26:30.386526Z",
     "iopub.status.idle": "2025-07-18T12:26:30.392240Z",
     "shell.execute_reply": "2025-07-18T12:26:30.391706Z"
    },
    "papermill": {
     "duration": 0.009671,
     "end_time": "2025-07-18T12:26:30.393307",
     "exception": false,
     "start_time": "2025-07-18T12:26:30.383636",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --- 3. FID CALCULATION LOGIC ---\n",
    "@torch.no_grad()\n",
    "def get_activations(dataloader, model, device, max_samples):\n",
    "    activations = []\n",
    "    for images, _ in dataloader:\n",
    "        if len(activations) * FID_BATCH_SIZE >= max_samples: break\n",
    "        images = images.to(device)\n",
    "        act = model(images)\n",
    "        activations.append(act.cpu())\n",
    "    return torch.cat(activations, dim=0)[:max_samples]\n",
    "\n",
    "def calculate_fid(act1, act2):\n",
    "    mu1, sigma1 = act1.mean(axis=0), np.cov(act1, rowvar=False)\n",
    "    mu2, sigma2 = act2.mean(axis=0), np.cov(act2, rowvar=False)\n",
    "    ssdiff = np.sum((mu1 - mu2)**2.0)\n",
    "    covmean = sqrtm(sigma1.dot(sigma2))\n",
    "    if np.iscomplexobj(covmean): covmean = covmean.real\n",
    "    fid = ssdiff + np.trace(sigma1 + sigma2 - 2.0 * covmean)\n",
    "    return fid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aa7e7ac2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-18T12:26:30.398584Z",
     "iopub.status.busy": "2025-07-18T12:26:30.398370Z",
     "iopub.status.idle": "2025-07-18T12:27:12.923087Z",
     "shell.execute_reply": "2025-07-18T12:27:12.921897Z"
    },
    "papermill": {
     "duration": 42.528838,
     "end_time": "2025-07-18T12:27:12.924346",
     "exception": false,
     "start_time": "2025-07-18T12:26:30.395508",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded trained generator from: /kaggle/input/cgan_200/pytorch/default/1/cgan_D1_G7_200_epochs/generator_final.pth\n",
      "Calculating activations for real images...\n",
      "Generating 1000 fake images and calculating activations...\n",
      "Calculating final FID score...\n",
      "\n",
      "========================================\n",
      "FID Score for '/kaggle/input/cgan_200/pytorch/default/1/cgan_D1_G7_200_epochs': 272.63\n",
      "========================================\n"
     ]
    }
   ],
   "source": [
    "# --- 4. DATA PREPARATION AND EXECUTION ---\n",
    "try:\n",
    "    gen_final = ConditionalGenerator(config.latent_dim, config.num_classes, config.embedding_dim, config.img_channels, config.features_g).to(DEVICE)\n",
    "    generator_path = os.path.join(experiment_dir_to_evaluate, 'generator_final.pth')\n",
    "    gen_final.load_state_dict(torch.load(generator_path))\n",
    "    gen_final.eval()\n",
    "    print(f\"Loaded trained generator from: {generator_path}\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"ERROR: Generator model not found at {generator_path}.\")\n",
    "    raise\n",
    "\n",
    "# --- Get REAL image activations ---\n",
    "real_dataset_fid = datasets.ImageFolder(root=config.dataset_path, transform=fid_transform)\n",
    "real_dataloader_fid = DataLoader(real_dataset_fid, batch_size=FID_BATCH_SIZE, shuffle=True, num_workers=config.num_workers)\n",
    "print(\"Calculating activations for real images...\")\n",
    "real_activations = get_activations(real_dataloader_fid, inception_model, DEVICE, max_samples=NUM_SAMPLES).numpy()\n",
    "\n",
    "# --- Get FAKE image activations ---\n",
    "fake_activations_list = []\n",
    "print(f\"Generating {NUM_SAMPLES} fake images and calculating activations...\")\n",
    "with torch.no_grad():\n",
    "    processed_samples = 0\n",
    "    while processed_samples < NUM_SAMPLES:\n",
    "        noise = torch.randn(FID_BATCH_SIZE, config.latent_dim, 1, 1, device=DEVICE)\n",
    "        labels = torch.randint(0, config.num_classes, (FID_BATCH_SIZE,), device=DEVICE)\n",
    "        generated_batch = gen_final(noise, labels) # Output is [-1, 1]\n",
    "        \n",
    "        # Apply the same FID transform to the generated images\n",
    "        # The transform expects PIL or tensor in [0,1] range, so we must denormalize\n",
    "        generated_batch_denorm = generated_batch * 0.5 + 0.5\n",
    "        \n",
    "        # We can apply the normalization part of the FID transform directly\n",
    "        generated_batch_transformed = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])(generated_batch_denorm)\n",
    "        # We assume the generator output is already 299x299 for FID, which isn't true.\n",
    "        # We must resize it first.\n",
    "        generated_batch_resized = nn.functional.interpolate(generated_batch_transformed, size=(FID_IMG_SIZE, FID_IMG_SIZE), mode='bilinear', align_corners=False)\n",
    "\n",
    "        act = inception_model(generated_batch_resized)\n",
    "        fake_activations_list.append(act.cpu())\n",
    "        processed_samples += FID_BATCH_SIZE\n",
    "\n",
    "fake_activations = torch.cat(fake_activations_list, dim=0)[:NUM_SAMPLES].numpy()\n",
    "\n",
    "# --- Calculate and Print Final Score ---\n",
    "print(\"Calculating final FID score...\")\n",
    "fid_score = calculate_fid(real_activations, fake_activations)\n",
    "\n",
    "print(\"\\n\" + \"=\"*40)\n",
    "print(f\"FID Score for '{experiment_dir_to_evaluate}': {fid_score:.2f}\")\n",
    "print(\"=\"*40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91c1b2d4",
   "metadata": {
    "papermill": {
     "duration": 0.002671,
     "end_time": "2025-07-18T12:27:12.930168",
     "exception": false,
     "start_time": "2025-07-18T12:27:12.927497",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96e091d3",
   "metadata": {
    "papermill": {
     "duration": 0.002117,
     "end_time": "2025-07-18T12:27:12.935152",
     "exception": false,
     "start_time": "2025-07-18T12:27:12.933035",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "486571f0",
   "metadata": {
    "papermill": {
     "duration": 0.002049,
     "end_time": "2025-07-18T12:27:12.939353",
     "exception": false,
     "start_time": "2025-07-18T12:27:12.937304",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "648307f9",
   "metadata": {
    "papermill": {
     "duration": 0.001996,
     "end_time": "2025-07-18T12:27:12.943588",
     "exception": false,
     "start_time": "2025-07-18T12:27:12.941592",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 7888962,
     "sourceId": 12499911,
     "sourceType": "datasetVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 402879,
     "modelInstanceId": 383433,
     "sourceId": 476765,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 402894,
     "modelInstanceId": 383449,
     "sourceId": 476784,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 403196,
     "modelInstanceId": 383774,
     "sourceId": 477195,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 60.972738,
   "end_time": "2025-07-18T12:27:14.666694",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-07-18T12:26:13.693956",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
