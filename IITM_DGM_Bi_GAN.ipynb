{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wduAVWSMZQrL"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9M7SgR25xYaY"
      },
      "outputs": [],
      "source": [
        "# ----------------------------\n",
        "# Configuration\n",
        "# ----------------------------\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "batch_size = 128\n",
        "latent_dim = 50\n",
        "epochs = 30\n",
        "lr = 2e-4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JyJT6yo5xeue"
      },
      "outputs": [],
      "source": [
        "# ----------------------------\n",
        "# Dataset (MNIST)\n",
        "# ----------------------------\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Lambda(lambda x: x.view(-1)),\n",
        "])\n",
        "\n",
        "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vEudc1SsxoMh"
      },
      "outputs": [],
      "source": [
        "# ----------------------------\n",
        "# Generator\n",
        "# ----------------------------\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(latent_dim, 1024),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(1024, 1024),\n",
        "            nn.BatchNorm1d(1024),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(1024, 784),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "    def forward(self, z):\n",
        "        return self.net(z)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "awT5ebumxsl4"
      },
      "outputs": [],
      "source": [
        "# ----------------------------\n",
        "# Encoder\n",
        "# ----------------------------\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(784, 1024),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(1024, 1024),\n",
        "            nn.BatchNorm1d(1024),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(1024, latent_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NzL2WzShxv74"
      },
      "outputs": [],
      "source": [
        "# ----------------------------\n",
        "# Discriminator (Joint: x, z)\n",
        "# ----------------------------\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(784 + latent_dim, 1024),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(1024, 1024),\n",
        "            nn.BatchNorm1d(1024),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(1024, 1)  # No sigmoid! BCEWithLogitsLoss handles that\n",
        "        )\n",
        "\n",
        "    def forward(self, x, z):\n",
        "        xz = torch.cat([x, z], dim=1)\n",
        "        return self.net(xz)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n2Z1AIx5xz4P"
      },
      "outputs": [],
      "source": [
        "# ----------------------------\n",
        "# Initialize models\n",
        "# ----------------------------\n",
        "G = Generator().to(device)\n",
        "E = Encoder().to(device)\n",
        "D = Discriminator().to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bD76MYmXx4eG"
      },
      "outputs": [],
      "source": [
        "# ----------------------------\n",
        "# Optimizers and Loss\n",
        "# ----------------------------\n",
        "bce_loss = nn.BCEWithLogitsLoss()\n",
        "optimizer_D = optim.Adam(D.parameters(), lr=lr, betas=(0.5, 0.999))\n",
        "optimizer_GE = optim.Adam(list(G.parameters()) + list(E.parameters()), lr=lr, betas=(0.5, 0.999))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "31W3_-_yzfX9"
      },
      "outputs": [],
      "source": [
        "k = 3 #\n",
        "p= 1\n",
        "import os\n",
        "from torchvision.utils import save_image\n",
        "\n",
        "os.makedirs(\"bigan\", exist_ok=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yf8hzbWxyAEE",
        "outputId": "9c4c4959-6123-4a5f-e478-617690cc72d4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/30] Batch 0/469 Loss D: 1.4097, Loss GE: 1.1159\n",
            "Epoch [1/30] Batch 100/469 Loss D: 0.5981, Loss GE: 0.7146\n",
            "Epoch [1/30] Batch 200/469 Loss D: 0.6035, Loss GE: 0.0037\n",
            "Epoch [1/30] Batch 300/469 Loss D: 0.2319, Loss GE: 0.0935\n",
            "Epoch [1/30] Batch 400/469 Loss D: 0.3863, Loss GE: 0.0086\n",
            "Epoch [2/30] Batch 0/469 Loss D: 0.5675, Loss GE: 0.5407\n",
            "Epoch [2/30] Batch 100/469 Loss D: 0.1829, Loss GE: 0.3675\n",
            "Epoch [2/30] Batch 200/469 Loss D: 0.0601, Loss GE: 0.1642\n",
            "Epoch [2/30] Batch 300/469 Loss D: 0.0486, Loss GE: 0.1234\n",
            "Epoch [2/30] Batch 400/469 Loss D: 0.3132, Loss GE: 3.4074\n",
            "Epoch [3/30] Batch 0/469 Loss D: 0.0482, Loss GE: 0.2699\n",
            "Epoch [3/30] Batch 100/469 Loss D: 0.0824, Loss GE: 0.5132\n",
            "Epoch [3/30] Batch 200/469 Loss D: 0.0284, Loss GE: 3.2051\n",
            "Epoch [3/30] Batch 300/469 Loss D: 0.0434, Loss GE: 8.1577\n",
            "Epoch [3/30] Batch 400/469 Loss D: 0.0237, Loss GE: 7.6635\n",
            "Epoch [4/30] Batch 0/469 Loss D: 0.0197, Loss GE: 7.1459\n",
            "Epoch [4/30] Batch 100/469 Loss D: 0.0150, Loss GE: 2.6315\n",
            "Epoch [4/30] Batch 200/469 Loss D: 0.0278, Loss GE: 6.7120\n",
            "Epoch [4/30] Batch 300/469 Loss D: 0.0179, Loss GE: 0.2049\n",
            "Epoch [4/30] Batch 400/469 Loss D: 0.0611, Loss GE: 7.7030\n",
            "Epoch [5/30] Batch 0/469 Loss D: 0.0213, Loss GE: 5.0149\n",
            "Epoch [5/30] Batch 100/469 Loss D: 0.0305, Loss GE: 6.3872\n",
            "Epoch [5/30] Batch 200/469 Loss D: 0.0122, Loss GE: 5.6930\n",
            "Epoch [5/30] Batch 300/469 Loss D: 0.2621, Loss GE: 9.6474\n",
            "Epoch [5/30] Batch 400/469 Loss D: 0.0088, Loss GE: 6.5885\n",
            "Epoch [6/30] Batch 0/469 Loss D: 0.0060, Loss GE: 11.0028\n",
            "Epoch [6/30] Batch 100/469 Loss D: 0.0250, Loss GE: 12.1217\n",
            "Epoch [6/30] Batch 200/469 Loss D: 0.0054, Loss GE: 13.4124\n",
            "Epoch [6/30] Batch 300/469 Loss D: 0.0141, Loss GE: 10.2329\n",
            "Epoch [6/30] Batch 400/469 Loss D: 0.0152, Loss GE: 2.5891\n",
            "Epoch [7/30] Batch 0/469 Loss D: 0.0095, Loss GE: 1.1433\n",
            "Epoch [7/30] Batch 100/469 Loss D: 0.0213, Loss GE: 4.4175\n",
            "Epoch [7/30] Batch 200/469 Loss D: 0.0244, Loss GE: 4.1924\n",
            "Epoch [7/30] Batch 300/469 Loss D: 0.0107, Loss GE: 4.2986\n",
            "Epoch [7/30] Batch 400/469 Loss D: 0.0103, Loss GE: 7.0609\n",
            "Epoch [8/30] Batch 0/469 Loss D: 0.0112, Loss GE: 4.6279\n",
            "Epoch [8/30] Batch 100/469 Loss D: 0.0071, Loss GE: 5.4619\n",
            "Epoch [8/30] Batch 200/469 Loss D: 0.0058, Loss GE: 3.3324\n",
            "Epoch [8/30] Batch 300/469 Loss D: 0.0082, Loss GE: 6.2018\n",
            "Epoch [8/30] Batch 400/469 Loss D: 0.0028, Loss GE: 7.2512\n",
            "Epoch [9/30] Batch 0/469 Loss D: 0.0027, Loss GE: 7.0628\n",
            "Epoch [9/30] Batch 100/469 Loss D: 0.0249, Loss GE: 4.6094\n",
            "Epoch [9/30] Batch 200/469 Loss D: 0.0097, Loss GE: 4.0487\n",
            "Epoch [9/30] Batch 300/469 Loss D: 0.0030, Loss GE: 6.6981\n",
            "Epoch [9/30] Batch 400/469 Loss D: 0.0332, Loss GE: 11.5094\n",
            "Epoch [10/30] Batch 0/469 Loss D: 0.0140, Loss GE: 9.7347\n",
            "Epoch [10/30] Batch 100/469 Loss D: 0.0230, Loss GE: 7.6772\n",
            "Epoch [10/30] Batch 200/469 Loss D: 0.0322, Loss GE: 5.1352\n",
            "Epoch [10/30] Batch 300/469 Loss D: 0.0307, Loss GE: 1.4447\n",
            "Epoch [10/30] Batch 400/469 Loss D: 0.0216, Loss GE: 8.3334\n",
            "Epoch [11/30] Batch 0/469 Loss D: 0.0156, Loss GE: 8.4964\n",
            "Epoch [11/30] Batch 100/469 Loss D: 0.0098, Loss GE: 8.7715\n",
            "Epoch [11/30] Batch 200/469 Loss D: 0.0061, Loss GE: 6.5455\n",
            "Epoch [11/30] Batch 300/469 Loss D: 0.0042, Loss GE: 6.7177\n",
            "Epoch [11/30] Batch 400/469 Loss D: 0.0263, Loss GE: 6.4520\n",
            "Epoch [12/30] Batch 0/469 Loss D: 0.0141, Loss GE: 2.7899\n",
            "Epoch [12/30] Batch 100/469 Loss D: 0.0209, Loss GE: 7.6671\n",
            "Epoch [12/30] Batch 200/469 Loss D: 0.0159, Loss GE: 8.7377\n",
            "Epoch [12/30] Batch 300/469 Loss D: 0.0070, Loss GE: 8.5400\n",
            "Epoch [12/30] Batch 400/469 Loss D: 0.0038, Loss GE: 8.6438\n",
            "Epoch [13/30] Batch 0/469 Loss D: 0.0058, Loss GE: 8.1450\n",
            "Epoch [13/30] Batch 100/469 Loss D: 0.0192, Loss GE: 5.1545\n",
            "Epoch [13/30] Batch 200/469 Loss D: 0.0092, Loss GE: 7.1076\n",
            "Epoch [13/30] Batch 300/469 Loss D: 0.0045, Loss GE: 7.4924\n",
            "Epoch [13/30] Batch 400/469 Loss D: 0.0280, Loss GE: 5.9594\n",
            "Epoch [14/30] Batch 0/469 Loss D: 0.0230, Loss GE: 4.6775\n",
            "Epoch [14/30] Batch 100/469 Loss D: 0.0101, Loss GE: 3.9360\n",
            "Epoch [14/30] Batch 200/469 Loss D: 0.0517, Loss GE: 3.8440\n",
            "Epoch [14/30] Batch 300/469 Loss D: 0.0099, Loss GE: 5.5659\n",
            "Epoch [14/30] Batch 400/469 Loss D: 0.0089, Loss GE: 5.2299\n",
            "Epoch [15/30] Batch 0/469 Loss D: 0.0120, Loss GE: 4.0998\n",
            "Epoch [15/30] Batch 100/469 Loss D: 0.0048, Loss GE: 4.1158\n",
            "Epoch [15/30] Batch 200/469 Loss D: 0.0030, Loss GE: 4.2006\n",
            "Epoch [15/30] Batch 300/469 Loss D: 0.0025, Loss GE: 4.3993\n",
            "Epoch [15/30] Batch 400/469 Loss D: 0.0109, Loss GE: 4.3961\n",
            "Epoch [16/30] Batch 0/469 Loss D: 0.0028, Loss GE: 4.2714\n",
            "Epoch [16/30] Batch 100/469 Loss D: 0.0029, Loss GE: 4.3565\n",
            "Epoch [16/30] Batch 200/469 Loss D: 0.0129, Loss GE: 4.4381\n",
            "Epoch [16/30] Batch 300/469 Loss D: 0.0022, Loss GE: 5.1016\n",
            "Epoch [16/30] Batch 400/469 Loss D: 0.0044, Loss GE: 4.7894\n",
            "Epoch [17/30] Batch 0/469 Loss D: 0.0436, Loss GE: 5.2978\n",
            "Epoch [17/30] Batch 100/469 Loss D: 0.0076, Loss GE: 3.3826\n",
            "Epoch [17/30] Batch 200/469 Loss D: 0.0338, Loss GE: 3.9741\n",
            "Epoch [17/30] Batch 300/469 Loss D: 0.0033, Loss GE: 3.7949\n",
            "Epoch [17/30] Batch 400/469 Loss D: 0.0023, Loss GE: 3.4104\n",
            "Epoch [18/30] Batch 0/469 Loss D: 0.0033, Loss GE: 3.7620\n",
            "Epoch [18/30] Batch 100/469 Loss D: 0.0018, Loss GE: 3.9553\n",
            "Epoch [18/30] Batch 200/469 Loss D: 0.0015, Loss GE: 3.8053\n",
            "Epoch [18/30] Batch 300/469 Loss D: 0.0022, Loss GE: 3.5474\n",
            "Epoch [18/30] Batch 400/469 Loss D: 0.0019, Loss GE: 3.5471\n",
            "Epoch [19/30] Batch 0/469 Loss D: 0.0013, Loss GE: 3.6706\n",
            "Epoch [19/30] Batch 100/469 Loss D: 0.0011, Loss GE: 3.6279\n",
            "Epoch [19/30] Batch 200/469 Loss D: 0.0014, Loss GE: 3.9359\n",
            "Epoch [19/30] Batch 300/469 Loss D: 0.0015, Loss GE: 3.7653\n",
            "Epoch [19/30] Batch 400/469 Loss D: 0.0045, Loss GE: 5.9974\n",
            "Epoch [20/30] Batch 0/469 Loss D: 0.0029, Loss GE: 4.9470\n",
            "Epoch [20/30] Batch 100/469 Loss D: 0.0021, Loss GE: 4.7912\n",
            "Epoch [20/30] Batch 200/469 Loss D: 0.0069, Loss GE: 5.5223\n",
            "Epoch [20/30] Batch 300/469 Loss D: 0.0129, Loss GE: 5.7392\n",
            "Epoch [20/30] Batch 400/469 Loss D: 0.0032, Loss GE: 5.2730\n",
            "Epoch [21/30] Batch 0/469 Loss D: 0.0033, Loss GE: 4.9183\n",
            "Epoch [21/30] Batch 100/469 Loss D: 0.0120, Loss GE: 10.8313\n",
            "Epoch [21/30] Batch 200/469 Loss D: 0.0203, Loss GE: 8.0869\n",
            "Epoch [21/30] Batch 300/469 Loss D: 0.0070, Loss GE: 2.2293\n",
            "Epoch [21/30] Batch 400/469 Loss D: 0.0026, Loss GE: 1.9965\n",
            "Epoch [22/30] Batch 0/469 Loss D: 0.0026, Loss GE: 2.1307\n",
            "Epoch [22/30] Batch 100/469 Loss D: 0.0025, Loss GE: 3.1819\n",
            "Epoch [22/30] Batch 200/469 Loss D: 0.0015, Loss GE: 5.4607\n",
            "Epoch [22/30] Batch 300/469 Loss D: 0.0034, Loss GE: 6.3946\n",
            "Epoch [22/30] Batch 400/469 Loss D: 0.0013, Loss GE: 5.3413\n",
            "Epoch [23/30] Batch 0/469 Loss D: 0.0009, Loss GE: 5.7744\n",
            "Epoch [23/30] Batch 100/469 Loss D: 0.0008, Loss GE: 7.1319\n",
            "Epoch [23/30] Batch 200/469 Loss D: 0.0036, Loss GE: 4.1900\n",
            "Epoch [23/30] Batch 300/469 Loss D: 0.0015, Loss GE: 5.2629\n",
            "Epoch [23/30] Batch 400/469 Loss D: 0.0012, Loss GE: 5.4073\n",
            "Epoch [24/30] Batch 0/469 Loss D: 0.0009, Loss GE: 5.2830\n",
            "Epoch [24/30] Batch 100/469 Loss D: 0.0009, Loss GE: 4.4445\n",
            "Epoch [24/30] Batch 200/469 Loss D: 0.0008, Loss GE: 4.4854\n",
            "Epoch [24/30] Batch 300/469 Loss D: 0.0009, Loss GE: 4.7987\n",
            "Epoch [24/30] Batch 400/469 Loss D: 0.0015, Loss GE: 4.6283\n",
            "Epoch [25/30] Batch 0/469 Loss D: 5.5627, Loss GE: 4.7975\n",
            "Epoch [25/30] Batch 100/469 Loss D: 0.0044, Loss GE: 4.9765\n",
            "Epoch [25/30] Batch 200/469 Loss D: 0.0019, Loss GE: 4.8212\n",
            "Epoch [25/30] Batch 300/469 Loss D: 0.0015, Loss GE: 3.1585\n",
            "Epoch [25/30] Batch 400/469 Loss D: 0.0023, Loss GE: 3.4541\n",
            "Epoch [26/30] Batch 0/469 Loss D: 0.0017, Loss GE: 4.2737\n",
            "Epoch [26/30] Batch 100/469 Loss D: 0.0008, Loss GE: 4.8836\n",
            "Epoch [26/30] Batch 200/469 Loss D: 0.0008, Loss GE: 4.6382\n"
          ]
        }
      ],
      "source": [
        "# ----------------------------\n",
        "# Training Loop\n",
        "# ----------------------------\n",
        "for epoch in range(1, epochs + 1):\n",
        "    for i, (x_real, _) in enumerate(train_loader):\n",
        "        x_real = x_real.to(device)\n",
        "        batch_size = x_real.size(0)\n",
        "\n",
        "        # Sample z from prior\n",
        "        z_real = torch.randn(batch_size, latent_dim, device=device)\n",
        "\n",
        "        # ----------------------\n",
        "        # 1. Train Discriminator\n",
        "        # ----------------------\n",
        "        for _ in range(p):\n",
        "          G.eval()\n",
        "          E.eval()\n",
        "          D.train()\n",
        "\n",
        "          x_fake = G(z_real).detach()\n",
        "          z_fake = E(x_real).detach()\n",
        "\n",
        "          D_real = D(x_real, z_fake)\n",
        "          D_fake = D(x_fake, z_real)\n",
        "\n",
        "          label_real = torch.ones_like(D_real)\n",
        "          label_fake = torch.zeros_like(D_fake)\n",
        "\n",
        "          loss_D = bce_loss(D_real, label_real) + bce_loss(D_fake, label_fake)\n",
        "\n",
        "          optimizer_D.zero_grad()\n",
        "          loss_D.backward()\n",
        "          optimizer_D.step()\n",
        "\n",
        "        # --------------------------\n",
        "        # 2. Train Generator + Encoder\n",
        "        # --------------------------\n",
        "        for _ in range(k):\n",
        "          G.train()\n",
        "          E.train()\n",
        "          D.eval()\n",
        "\n",
        "          x_fake = G(z_real)\n",
        "          z_fake = E(x_real)\n",
        "\n",
        "          D_real = D(x_real, z_fake)\n",
        "          D_fake = D(x_fake, z_real)\n",
        "\n",
        "          loss_GE = bce_loss(D_real, label_fake) + bce_loss(D_fake, label_real)\n",
        "\n",
        "          optimizer_GE.zero_grad()\n",
        "          loss_GE.backward()\n",
        "          optimizer_GE.step()\n",
        "\n",
        "        if i % 100 == 0:\n",
        "            print(f\"Epoch [{epoch}/{epochs}] Batch {i}/{len(train_loader)}\",\n",
        "                  f\"Loss D: {loss_D.item():.4f}, Loss GE: {loss_GE.item():.4f}\")\n",
        "\n",
        "        # Save samples every epoch\n",
        "        G.eval()\n",
        "        with torch.no_grad():\n",
        "            z = torch.randn(64, latent_dim, device=device)\n",
        "            samples = G(z)\n",
        "            samples = samples * 0.5 + 0.5  # Denormalize\n",
        "            save_image(samples, f\"bigan/epoch_{epoch}.png\", nrow=8)\n",
        "        G.train()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_dN5TG3VyGzG"
      },
      "outputs": [],
      "source": [
        "# ----------------------------\n",
        "# Generate Samples for Visualization\n",
        "# ----------------------------\n",
        "def show_samples():\n",
        "    G.eval()\n",
        "    with torch.no_grad():\n",
        "        z = torch.randn(64, latent_dim, device=device)\n",
        "        x_gen = G(z).view(-1, 1, 28, 28).cpu()\n",
        "        grid = torchvision.utils.make_grid(x_gen, nrow=8)\n",
        "        plt.imshow(grid.permute(1, 2, 0).squeeze())\n",
        "        plt.axis('off')\n",
        "        plt.title(\"Generated Samples from BiGAN\")\n",
        "        plt.show()\n",
        "\n",
        "show_samples()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}